# Bounzle - Endless Bouncer

A zesty one-tap endless bouncer with AI-procedural level generation, built with Next.js, TypeScript, and HTML5 Canvas.

![Bounzle Gameplay](public/screenshots/gameplay.png)

## üéÆ About

Bounzle is a modern take on the classic endless bouncer genre. Tap or click to make the ball jump and navigate through procedurally generated gaps. With AI-powered level generation, dynamic themes, particle effects, and progressive difficulty, every playthrough is unique.

## ‚ú® Features

- **One-Tap Controls**: Simple and addictive gameplay
- **AI-Procedural Levels**: Unique levels generated by Groq AI
- **Dynamic Themes**: Visual themes that change as you play
- **Particle Effects**: Juicy visual feedback for all actions
- **Progressive Difficulty**: Game gets progressively harder
- **Sound Effects**: Immersive audio experience
- **High Scores**: Global leaderboard with Supabase integration
- **Monetization Ready**: Banner ads and rewarded video placeholders
- **PWA Support**: Installable on mobile devices
- **Offline Play**: Works offline after first load

## üõ†Ô∏è Tech Stack

- **Frontend**: Next.js 15 (App Router), TypeScript, Tailwind CSS
- **Game Engine**: Custom HTML5 Canvas implementation
- **AI Generation**: Groq API (Llama 3.3) or Local Ollama (for development)
- **Database**: Supabase (PostgreSQL)
- **Authentication**: Supabase Auth
- **Deployment**: Vercel
- **Monetization**: AdMob (placeholder/real implementation)

## üöÄ Getting Started

### Prerequisites

- Node.js 18+
- npm or yarn
- Supabase account
- Groq API key (or local Ollama instance for development)
- AdMob account (for monetization)

### Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/bounzle.git
cd bounzle/bounzle-web
```

2. Install dependencies:
```bash
npm install
```

3. Set up environment variables:
Create a `.env.local` file in the `bounzle-web` directory:
```env
NEXT_PUBLIC_SUPABASE_URL=your_supabase_url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anon_key
GROQ_API_KEY=your_groq_api_key

# Optional: Use local LLM for development
# Set USE_LOCAL_LLM=true to use a local LLM instance instead of Groq API
USE_LOCAL_LLM=false
LOCAL_LLM_PROVIDER=lmstudio  # Options: lmstudio (preferred), nvidia-nim, ollama
LOCAL_LLM_URL=http://localhost:1234  # LM Studio default, or http://localhost:11434 for Ollama, or https://integrate.api.nvidia.com/v1 for NVIDIA NIM Cloud
LOCAL_LLM_MODEL=llama-3.3-70b-versatile  # Model name (varies by provider)
LOCAL_LLM_API_KEY=  # Required for NVIDIA NIM Cloud (NGC API key)
```

4. Run the development server:
```bash
npm run dev
```

5. Open [http://localhost:3000](http://localhost:3000) in your browser.

## üéØ How to Play

1. Click or tap the screen to start
2. Tap to make the ball jump and avoid obstacles
3. Navigate through the gaps in the obstacles
4. Try to achieve the highest score possible
5. Watch for theme changes every 500 points
6. Use rewarded ads to gain extra time when close to dying

## üèÜ High Scores

Visit the [Leaderboard](/leaderboard) to see global high scores and compete with other players.

## üí∞ Monetization with AdMob

Bounzle includes both banner ads and rewarded video ads:

### Banner Ads
- Displayed at the bottom of the screen during gameplay
- Using Google AdMob test ad units by default
- Can be easily configured with your own ad units

### Rewarded Ads
- Available on the game over screen
- Players can watch ads to earn extra time (+5 seconds)
- Implemented with proper reward callback system

### Setting up Real AdMob Ads

1. Create an AdMob account at [https://admob.google.com](https://admob.google.com)
2. Create apps for Android and/or iOS
3. Create ad units for:
   - Banner ads
   - Rewarded video ads
4. Replace the test ad unit IDs in `src/lib/admob.ts` with your real ad unit IDs:
   ```typescript
   // Replace these test IDs with your real AdMob ad unit IDs
   private bannerAdUnitId: string = 'YOUR_BANNER_AD_UNIT_ID';
   private rewardedAdUnitId: string = 'YOUR_REWARDED_AD_UNIT_ID';
   ```
5. For web deployment, ensure you've added your domain to the AdMob allowlist

## üì± Progressive Web App

Bounzle works as a PWA and can be installed on mobile devices:
1. Open the game in Chrome on Android or Safari on iOS
2. Look for the install prompt or use the browser's menu to "Add to Home Screen"
3. Play the game offline anytime!

## üß† Local LLM Setup (Development)

For development, you can use a local LLM instance instead of the Groq API. This is useful for:
- Testing without API costs
- Faster iteration during development
- Working offline

### Setting up LM Studio (Preferred)

1. Download and install LM Studio from [https://lmstudio.ai](https://lmstudio.ai)

2. Download a compatible model (e.g., `llama-3.3-70b-versatile` or `Meta-Llama-3.3-70B-Instruct`)

3. Start the local server in LM Studio:
   - Open LM Studio
   - Go to the "Local Server" tab
   - Click "Start Server" (defaults to `http://localhost:1234`)

4. Configure your `.env.local`:
   ```env
   USE_LOCAL_LLM=true
   LOCAL_LLM_PROVIDER=lmstudio
   LOCAL_LLM_URL=http://localhost:1234
   LOCAL_LLM_MODEL=llama-3.3-70b-versatile
   ```

### Setting up NVIDIA NIM Cloud

1. **Get your NGC API Key:**
   - Sign up for a free NVIDIA Cloud account at [https://build.nvidia.com](https://build.nvidia.com)
   - Navigate to your account settings to generate an NGC API key
   - Copy the API key for use in your environment variables

2. **Find the Model Endpoint:**
   - Visit the [NVIDIA API Catalog](https://build.nvidia.com) and search for "llama-3.3-70b" or "llama-3.3-70b-instruct"
   - The model page will show the exact API endpoint URL
   - Common endpoint format: `https://integrate.api.nvidia.com/v1`
   - Model names typically follow: `meta/llama-3.3-70b-instruct` or `meta/llama-3.3-70b-versatile`

3. **Configure your `.env.local`:**
   ```env
   USE_LOCAL_LLM=true
   LOCAL_LLM_PROVIDER=nvidia-nim
   LOCAL_LLM_URL=https://integrate.api.nvidia.com/v1
   LOCAL_LLM_MODEL=meta/llama-3.3-70b-instruct
   LOCAL_LLM_API_KEY=your_ngc_api_key_here
   ```

   **Note:** Replace `meta/llama-3.3-70b-instruct` with the exact model name from the NVIDIA API Catalog. The model name format may vary.

### Setting up Ollama

1. Install Ollama from [https://ollama.ai](https://ollama.ai)

2. Pull the llama-3.3-70b model (or a compatible model):
   ```bash
   ollama pull llama3.3:70b
   ```
   
   Note: The 70B model requires significant RAM (40GB+). For development, you can use smaller models:
   ```bash
   ollama pull llama3.3:8b  # Much smaller, faster for dev
   ```

3. Start Ollama (it runs on `http://localhost:11434` by default)

4. Configure your `.env.local`:
   ```env
   USE_LOCAL_LLM=true
   LOCAL_LLM_PROVIDER=ollama
   LOCAL_LLM_URL=http://localhost:11434
   LOCAL_LLM_MODEL=llama3.3:70b  # or llama3.3:8b for faster dev
   ```

### Supported Local LLM Providers

- **LM Studio** (preferred): Easy-to-use GUI, OpenAI-compatible API
- **NVIDIA NIM**: High-performance inference with API key support
- **Ollama**: Command-line tool with simple setup

The application will automatically use the local LLM when `USE_LOCAL_LLM=true` is set.

### Quick Reference: NVIDIA NIM Cloud Configuration

**API Endpoint:** `https://integrate.api.nvidia.com/v1`

**Available Models (check [build.nvidia.com](https://build.nvidia.com) for latest):**
- `meta/llama-3.3-70b-instruct` (recommended)
- `meta/llama-3.3-70b-versatile` (if available)
- `meta/llama-3.1-70b-instruct` (alternative)

**Example `.env.local` for NVIDIA NIM Cloud:**
```env
USE_LOCAL_LLM=true
LOCAL_LLM_PROVIDER=nvidia-nim
LOCAL_LLM_URL=https://integrate.api.nvidia.com/v1
LOCAL_LLM_MODEL=meta/llama-3.3-70b-instruct
LOCAL_LLM_API_KEY=your_ngc_api_key_from_build.nvidia.com
```

**Getting your NGC API Key:**
1. Visit [https://build.nvidia.com](https://build.nvidia.com)
2. Sign in or create a free account
3. Go to your account settings ‚Üí API Keys
4. Generate a new API key
5. Copy it to `LOCAL_LLM_API_KEY` in your `.env.local`

## üöÄ Deployment

### Deploy to Vercel (Recommended)

1. Push your code to GitHub/GitLab/Bitbucket
2. Sign up for a Vercel account at [https://vercel.com](https://vercel.com)
3. Create a new project and import your repository
4. Set environment variables in the Vercel dashboard:
   - `NEXT_PUBLIC_SUPABASE_URL`
   - `NEXT_PUBLIC_SUPABASE_ANON_KEY`
   - `GROQ_API_KEY`
   - (Optional) `USE_LOCAL_LLM`, `LOCAL_LLM_URL`, `LOCAL_LLM_MODEL` for local development
5. Deploy!

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/yourusername/bounzle)

### Deploy to Netlify

1. Push your code to GitHub/GitLab/Bitbucket
2. Sign up for a Netlify account at [https://netlify.com](https://netlify.com)
3. Create a new site from Git
4. Set build command to `next build`
5. Set publish directory to `.next/standalone`
6. Set environment variables in the Netlify dashboard

### Self-hosted Deployment

1. Build the project:
   ```bash
   cd bounzle/bounzle-web
   npm run build
   ```
2. The built files will be in the `.next` directory
3. Serve the files using any web server (nginx, Apache, etc.)
4. Configure your web server for static file serving

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôå Acknowledgements

- Sound effects from [Freesound.org](https://freesound.org/)
- Icons made by [Font Awesome](https://fontawesome.com/)
- AI level generation powered by [Groq](https://groq.com/)
- Backend services powered by [Supabase](https://supabase.com/)
- Monetization powered by [Google AdMob](https://admob.google.com/)

## üåê Live Demo

Play the game at: [https://bounzle.vercel.app](https://bounzle.vercel.app)

*Note: This is a placeholder URL. Update with your actual deployment URL.*